{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import os\n",
    "from typing import Annotated\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    get_buffer_string,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "from langgraph.types import Send\n",
    "from pydantic import BaseModel, Field\n",
    "from serpapi import GoogleSearch\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "\n",
    "def load_prompt(prompt_filename, partial_variables=None):\n",
    "    partial_variables = partial_variables or {}\n",
    "    with open(prompt_filename, \"r\") as f:\n",
    "        file_content = f.read()\n",
    "        return PromptTemplate.from_template(\n",
    "            file_content, template_format=\"jinja2\", partial_variables=partial_variables\n",
    "        ).format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterviewState(MessagesState):\n",
    "    max_num_turns: int\n",
    "    context: Annotated[list, operator.add]\n",
    "    interview: str\n",
    "    sections: list\n",
    "\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Search query for retrieval.\")\n",
    "\n",
    "\n",
    "def generate_question(state: InterviewState):\n",
    "    messages = state[\"messages\"]\n",
    "    analyst_system_prompt = load_prompt(\n",
    "        \"./assets/15_complex_agent/analyst_system_prompt.jinja2\"\n",
    "    )\n",
    "    question = llm.invoke([SystemMessage(content=analyst_system_prompt)] + messages)\n",
    "    return {\"messages\": [question]}\n",
    "\n",
    "\n",
    "def search_web(state: InterviewState):\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_system_prompt = load_prompt(\n",
    "        \"assets/15_complex_agent/search_system_prompt.jinja2\"\n",
    "    )\n",
    "    search_query = structured_llm.invoke([search_system_prompt] + state[\"messages\"])\n",
    "\n",
    "    params = {\n",
    "        \"q\": search_query.search_query,\n",
    "        \"hl\": \"en\",\n",
    "        \"google_domain\": \"google.com\",\n",
    "        \"api_key\": os.getenv(\"SERPAPI_API_KEY\"),\n",
    "    }\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    organic_results = results[\"organic_results\"] if \"organic_results\" in results else []\n",
    "\n",
    "    formatted_results = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document source=\"{doc[\"link\"]}\" page=\"{doc[\"title\"]}\"/>\\n{doc[\"snippet\"]}\\n</Document>'\n",
    "            for doc in organic_results\n",
    "        ]\n",
    "    )\n",
    "    return {\"context\": [formatted_results]}\n",
    "\n",
    "\n",
    "def search_wikipedia(state: InterviewState):\n",
    "    structured_llm = llm.with_structured_output(SearchQuery)\n",
    "    search_system_prompt = load_prompt(\n",
    "        \"assets/15_complex_agent/search_system_prompt.jinja2\"\n",
    "    )\n",
    "    search_query = structured_llm.invoke([search_system_prompt] + state[\"messages\"])\n",
    "\n",
    "    search_docs = WikipediaLoader(\n",
    "        query=search_query.search_query, load_max_docs=2\n",
    "    ).load()\n",
    "\n",
    "    formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document source=\"{doc.metadata[\"source\"]}\" page=\"{doc.metadata.get(\"page\", \"\")}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in search_docs\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"context\": [formatted_search_docs]}\n",
    "\n",
    "\n",
    "def generate_answer(state: InterviewState):\n",
    "    expert_system_prompt = load_prompt(\n",
    "        \"assets/15_complex_agent/expert_system_prompt.jinja2\",\n",
    "        {\"context\": state[\"context\"]},\n",
    "    )\n",
    "    answer = llm.invoke(\n",
    "        [SystemMessage(content=expert_system_prompt)] + state[\"messages\"]\n",
    "    )\n",
    "    answer.name = \"expert\"\n",
    "\n",
    "    return {\"messages\": [answer]}\n",
    "\n",
    "\n",
    "def save_interview(state: InterviewState):\n",
    "    messages = state[\"messages\"]\n",
    "    interview = get_buffer_string(messages)\n",
    "    return {\"interview\": interview}\n",
    "\n",
    "\n",
    "def route_messages(state: InterviewState, name: str = \"expert\"):\n",
    "    messages = state[\"messages\"]\n",
    "    max_num_turns = state.get(\"max_num_turns\", 2)\n",
    "    num_responses = len(\n",
    "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
    "    )\n",
    "\n",
    "    if num_responses >= max_num_turns:\n",
    "        return \"save_interview\"\n",
    "\n",
    "    last_question = messages[-2]\n",
    "\n",
    "    if \"Thank you. That's all.\" in last_question.content:\n",
    "        return \"save_interview\"\n",
    "    return \"ask_question\"\n",
    "\n",
    "\n",
    "def write_section(state: InterviewState):\n",
    "    context = state[\"context\"]\n",
    "    writer_system_prompt = load_prompt(\n",
    "        \"assets/15_complex_agent/section_writer_system_prompt.jinja2\"\n",
    "    )\n",
    "    section = llm.invoke(\n",
    "        [SystemMessage(content=writer_system_prompt)]\n",
    "        + [HumanMessage(content=f\"Use this source to write your section: {context}\")]\n",
    "    )\n",
    "    return {\"sections\": [section.content]}\n",
    "\n",
    "\n",
    "interview_builder = StateGraph(InterviewState)\n",
    "interview_builder.add_node(\"ask_question\", generate_question)\n",
    "interview_builder.add_node(\"search_web\", search_web)\n",
    "interview_builder.add_node(\"search_wikipedia\", search_wikipedia)\n",
    "interview_builder.add_node(\"answer_question\", generate_answer)\n",
    "interview_builder.add_node(\"save_interview\", save_interview)\n",
    "interview_builder.add_node(\"write_section\", write_section)\n",
    "\n",
    "interview_builder.add_edge(START, \"ask_question\")\n",
    "interview_builder.add_edge(\"ask_question\", \"search_web\")\n",
    "interview_builder.add_edge(\"ask_question\", \"search_wikipedia\")\n",
    "interview_builder.add_edge(\"search_web\", \"answer_question\")\n",
    "interview_builder.add_edge(\"search_wikipedia\", \"answer_question\")\n",
    "interview_builder.add_conditional_edges(\n",
    "    \"answer_question\", route_messages, [\"ask_question\", \"save_interview\"]\n",
    ")\n",
    "interview_builder.add_edge(\"save_interview\", \"write_section\")\n",
    "interview_builder.add_edge(\"write_section\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "interview_agent = interview_builder.compile(checkpointer=memory)\n",
    "\n",
    "# View\n",
    "display(Image(interview_agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "message = HumanMessage(content=\"So you said you were writing an article on LangGraph?\")\n",
    "interview_agent.invoke({\"messages\": [message]}, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchGraphState(TypedDict):\n",
    "    topic: str\n",
    "    analysts: int\n",
    "    sections: Annotated[list, operator.add]\n",
    "    introduction: str\n",
    "    content: str\n",
    "    conclusion: str\n",
    "    final_report: str\n",
    "\n",
    "\n",
    "def initiate_all_interviews(state: ResearchGraphState):\n",
    "    topic = state[\"topic\"]\n",
    "    return [\n",
    "        Send(\n",
    "            \"conduct_interview\",\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    HumanMessage(\n",
    "                        content=f\"So you said you were writing an article on {topic}?\"\n",
    "                    )\n",
    "                ]\n",
    "            },\n",
    "        )\n",
    "        for _ in range(state[\"analysts\"])\n",
    "    ]\n",
    "\n",
    "\n",
    "def write_report(state: ResearchGraphState):\n",
    "    formatted_str_sections = \"\\n\\n\".join(\n",
    "        [f\"{section}\" for section in state[\"sections\"]]\n",
    "    )\n",
    "    system_message = load_prompt(\n",
    "        \"assets/15_complex_agent/report_writer_system_prompt.jinja2\",\n",
    "        {\"topic\": state[\"topic\"], \"context\": formatted_str_sections},\n",
    "    )\n",
    "    report = llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=\"Write a report based upon these memos.\")]\n",
    "    )\n",
    "    return {\"content\": report.content}\n",
    "\n",
    "\n",
    "def write_introduction(state: ResearchGraphState):\n",
    "    formatted_str_sections = \"\\n\\n\".join(\n",
    "        [f\"{section}\" for section in state[\"sections\"]]\n",
    "    )\n",
    "    system_message = load_prompt(\n",
    "        \"assets/15_complex_agent/intro_conclusion_system_prompt.jinja2\",\n",
    "        {\"topic\": state[\"topic\"], \"formatted_str_sections\": formatted_str_sections},\n",
    "    )\n",
    "    intro = llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=\"Write the report introduction\")]\n",
    "    )\n",
    "    return {\"introduction\": intro.content}\n",
    "\n",
    "\n",
    "def write_conclusion(state: ResearchGraphState):\n",
    "    formatted_str_sections = \"\\n\\n\".join(\n",
    "        [f\"{section}\" for section in state[\"sections\"]]\n",
    "    )\n",
    "    system_message = load_prompt(\n",
    "        \"assets/15_complex_agent/intro_conclusion_system_prompt.jinja2\",\n",
    "        {\"topic\": state[\"topic\"], \"formatted_str_sections\": formatted_str_sections},\n",
    "    )\n",
    "    conclusion = llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=\"Write the report conclusion\")]\n",
    "    )\n",
    "    return {\"conclusion\": conclusion.content}\n",
    "\n",
    "\n",
    "def finalize_report(state: ResearchGraphState):\n",
    "    content = state[\"content\"]\n",
    "    if content.startswith(\"## Insights\"):\n",
    "        content = content.strip(\"## Insights\")\n",
    "    if \"## Sources\" in content:\n",
    "        try:\n",
    "            content, sources = content.split(\"\\n## Sources\\n\")\n",
    "        except Exception:\n",
    "            sources = None\n",
    "    else:\n",
    "        sources = None\n",
    "\n",
    "    final_report = (\n",
    "        state[\"introduction\"]\n",
    "        + \"\\n\\n---\\n\\n\"\n",
    "        + content\n",
    "        + \"\\n\\n---\\n\\n\"\n",
    "        + state[\"conclusion\"]\n",
    "    )\n",
    "    if sources is not None:\n",
    "        final_report += \"\\n\\n## Sources\\n\" + sources\n",
    "    return {\"final_report\": final_report}\n",
    "\n",
    "\n",
    "builder = StateGraph(ResearchGraphState)\n",
    "builder.add_node(\"conduct_interview\", interview_builder.compile())\n",
    "builder.add_node(\"write_report\", write_report)\n",
    "builder.add_node(\"write_introduction\", write_introduction)\n",
    "builder.add_node(\"write_conclusion\", write_conclusion)\n",
    "builder.add_node(\"finalize_report\", finalize_report)\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    source=START, path=initiate_all_interviews, path_map=[\"conduct_interview\"]\n",
    ")\n",
    "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_introduction\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_conclusion\")\n",
    "builder.add_edge(\n",
    "    [\"write_conclusion\", \"write_report\", \"write_introduction\"], \"finalize_report\"\n",
    ")\n",
    "builder.add_edge(\"finalize_report\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.invoke(\n",
    "    {\"topic\": \"LangGraph\", \"analysts\": 4},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}, \"max_concurrency\": 15},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
