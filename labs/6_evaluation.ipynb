{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import Client\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"AI-MO/aimo-validation-aime\")\n",
    "examples = [\n",
    "    {\"inputs\": {\"question\": d[\"problem\"]}, \"outputs\": {\"answer\": int(d[\"answer\"])}}\n",
    "    for d in ds[\"train\"]\n",
    "][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "\n",
    "dataset_name = \"AIME Example Dataset (sample)\"\n",
    "\n",
    "try:\n",
    "    dataset = client.create_dataset(dataset_name)\n",
    "    client.create_examples(dataset_id=dataset.id, examples=examples)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response(BaseModel):\n",
    "    explanation: str = Field(description=\"The explanation of the answer\")\n",
    "    answer: int = Field(\n",
    "        description=\"The answer to the question. It should be an integer.\"\n",
    "    )\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "model_with_structure = model.with_structured_output(Response, method=\"function_calling\")\n",
    "\n",
    "\n",
    "def get_response(question: str) -> Response:\n",
    "    max_retries = 3\n",
    "    for _ in range(max_retries):\n",
    "        try:\n",
    "            messages = [\n",
    "                SystemMessage(\n",
    "                    \"You're a math expert. You will always respond in a JSON format with the following fields: explanation and answer.\"\n",
    "                ),\n",
    "                HumanMessage(question),\n",
    "            ]\n",
    "            response = model_with_structure.invoke(messages)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    raise ValueError(\"Failed to get a valid response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_wrapper(inputs: str) -> dict:\n",
    "    response = get_response(inputs[\"question\"])\n",
    "    return response.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    return outputs[\"answer\"] == reference_outputs[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_results = client.evaluate(\n",
    "#     ls_wrapper, data=dataset_name, evaluators=[accuracy], max_concurrency=15\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Create an LLM judge that evaluates if the answer is accurate and the clarity of the explanation of the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"ChilleD/LastLetterConcat\")\n",
    "examples = [\n",
    "    {\"inputs\": {\"question\": d[\"question\"]}, \"outputs\": {\"answer\": d[\"answer\"]}}\n",
    "    for d in ds[\"train\"]\n",
    "][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "\n",
    "dataset_name = \"LastLetterConcat Example Dataset (sample)\"\n",
    "\n",
    "try:\n",
    "    dataset = client.create_dataset(dataset_name)\n",
    "    client.create_examples(dataset_id=dataset.id, examples=examples)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Response(BaseModel):\n",
    "    explanation: str = Field(description=\"The explanation of the answer\")\n",
    "    answer: str = Field(\n",
    "        description=\"The answer to the question. It should be a string with 4 characters.\",\n",
    "        pattern=r\"^[a-zA-Z]{4}$\",\n",
    "    )\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "model_with_structure = model.with_structured_output(Response, method=\"function_calling\")\n",
    "\n",
    "\n",
    "def get_response(question: str) -> Response:\n",
    "    max_retries = 3\n",
    "    for _ in range(max_retries):\n",
    "        try:\n",
    "            messages = [\n",
    "                SystemMessage(\n",
    "                    \"You're a puzzle expert. You will always respond in a JSON format with the following fields: explanation and answer.\"\n",
    "                ),\n",
    "                HumanMessage(question),\n",
    "            ]\n",
    "            response = model_with_structure.invoke(messages)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "    raise ValueError(\"Failed to get a valid response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clarity(BaseModel):\n",
    "    explanation: str = Field(description=\"The explanation of the answer\")\n",
    "    clarity: int = Field(description=\"The clarity of the explanation\", ge=1, le=5)\n",
    "\n",
    "\n",
    "def clarity(inputs: dict, outputs: dict, reference_outputs: dict) -> int:\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that evaluates the clarity of the explanation of the answer. You will always return a number between 1 and 5, where 1 is the lowest clarity and 5 is the highest clarity.\"\n",
    "        ),\n",
    "        HumanMessage(content=f\"Explanation: {outputs['explanation']}\"),\n",
    "    ]\n",
    "    model_with_clarity_structure = model.with_structured_output(Clarity)\n",
    "    response = model_with_clarity_structure.invoke(messages)\n",
    "    return response.clarity\n",
    "\n",
    "\n",
    "def accuracy(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    return outputs[\"answer\"] == reference_outputs[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_results = client.evaluate(\n",
    "#     ls_wrapper, data=dataset_name, evaluators=[accuracy, clarity], max_concurrency=15\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
