{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import chromadb\n",
    "import tiktoken\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_text_splitters import (\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"assets/bbva.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "pages = []\n",
    "\n",
    "for page in loader.lazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that can answer questions about the provided context.\n",
    "\n",
    "Please cite the page number used to answer the question. Write the page number in the format \"Page X\" at the end of your answer. \n",
    "\n",
    "If the answer is not found in the context, please say so.\n",
    "\"\"\"\n",
    "user_message = \"\"\"\n",
    "Please answer the following question based on the context provided:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\"\"\"\n",
    "\n",
    "messages = [SystemMessage(content=system_prompt), HumanMessage(content=user_message)]\n",
    "context = \"\"\n",
    "for i, page in enumerate(pages):\n",
    "    context += f\"--- PAGE {i + 1} ---\\n{page.page_content}\\n\\n\"\n",
    "\n",
    "\n",
    "def get_response(context: dict):\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_message.format(**context)),\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "question = \"What is the main idea of the document?\"\n",
    "response = get_response({\"question\": question, \"documents\": context})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the daily transaction limits?\"\n",
    "response = get_response({\"question\": question, \"documents\": context})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many cards can I have?\"\n",
    "response = get_response({\"question\": question, \"documents\": context})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ef = OpenAIEmbeddingFunction(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "vector_db = chromadb.PersistentClient()\n",
    "\n",
    "try:\n",
    "    collection = vector_db.delete_collection(\"bbva\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "collection = vector_db.create_collection(\"bbva\", embedding_function=openai_ef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and index documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=[split.page_content for split in all_splits],\n",
    "    metadatas=[split.metadata for split in all_splits],\n",
    "    ids=[str(i) for i in range(len(all_splits))],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What are the customer service channels?\"\n",
    "embeddings = openai_ef.embed_with_retries(text)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.query(query_texts=[\"What are the customer service channels?\"], n_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "text_1 = \"What are the customer service channels?\"\n",
    "text_2 = \"What are the customer service channels?\"\n",
    "\n",
    "embeddings_1 = openai_ef.embed_with_retries(text_1)[0]\n",
    "embeddings_2 = openai_ef.embed_with_retries(text_2)[0]\n",
    "\n",
    "cos_sim = np.dot(embeddings_1, embeddings_2) / (\n",
    "    np.linalg.norm(embeddings_1) * np.linalg.norm(embeddings_2)\n",
    ")\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.query(query_embeddings=embeddings, n_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.query(\n",
    "    query_texts=[\"What are the daily transaction limits?\", \"Is there a monthly limit?\"],\n",
    "    n_results=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that can answer questions about the provided context.\n",
    "\n",
    "Please cite the page number used to answer the question. Write the page number in the format \"Page X\" at the end of your answer. \n",
    "\n",
    "If the answer is not found in the context, please say so.\n",
    "\"\"\"\n",
    "\n",
    "user_message = \"\"\"\n",
    "Please answer the following question based on the context provided:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@traceable\n",
    "def get_relevant_docs(question: str):\n",
    "    relevant_docs = collection.query(query_texts=question, n_results=3)\n",
    "    documents = relevant_docs[\"documents\"][0]\n",
    "    metadatas = relevant_docs[\"metadatas\"][0]\n",
    "    return [\n",
    "        {\"page_content\": doc, \"type\": \"Document\", \"metadata\": metadata}\n",
    "        for doc, metadata in zip(documents, metadatas)\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_context(relevant_docs: list[dict]):\n",
    "    context = \"\"\n",
    "    for doc in relevant_docs:\n",
    "        context += f\"--- PAGE {doc['metadata']['page']} ---\\n{doc['page_content']}\\n\\n\"\n",
    "    return context\n",
    "\n",
    "\n",
    "@traceable\n",
    "def get_messages(question: str, relevant_docs: dict):\n",
    "    prompt_vars = {\"question\": question, \"documents\": get_context(relevant_docs)}\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_message.format(**prompt_vars)),\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "\n",
    "@traceable\n",
    "def get_response(question: str):\n",
    "    relevant_docs = get_relevant_docs(question)\n",
    "    messages = get_messages(question, relevant_docs)\n",
    "    response = model.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "question = \"What are the customer service channels?\"\n",
    "response = get_response(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise\n",
    "\n",
    "Download this book and create a vector database with it: https://github.com/mlschmitt/classic-books-markdown/blob/main/Friedrich%20Nietzsche/Beyond%20Good%20and%20Evil.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"assets/book.md\")\n",
    "book_text = loader.load()[0].page_content\n",
    "\n",
    "md_text_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=[(\"#\", \"header_1\"), (\"##\", \"header_2\")]\n",
    ")\n",
    "all_splits = md_text_splitter.split_text(book_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ef = OpenAIEmbeddingFunction(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "vector_db = chromadb.PersistentClient()\n",
    "\n",
    "try:\n",
    "    vector_db.delete_collection(\"book\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "collection = vector_db.create_collection(\"book\", embedding_function=openai_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    collection.add(\n",
    "        documents=[s.page_content for s in all_splits],\n",
    "        ids=[str(i) for i in range(len(all_splits))],\n",
    "        metadatas=[s.metadata for s in all_splits],\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "tokens = [len(encoding.encode(s.page_content)) for s in all_splits]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_n_tokens(text, n):\n",
    "    encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "    tokens = encoding.encode(text)\n",
    "    text_tokens = encoding.decode(tokens[:n])\n",
    "    return text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(\n",
    "    documents=[get_first_n_tokens(s.page_content, 8191) for s in all_splits],\n",
    "    ids=[str(i) for i in range(len(all_splits))],\n",
    "    metadatas=[s.metadata for s in all_splits],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_tokens(text):\n",
    "    encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=8192,\n",
    "    separators=[\"\\n\\n\", \"\\n\"],\n",
    "    chunk_overlap=0,\n",
    "    length_function=get_num_tokens,\n",
    ")\n",
    "\n",
    "adjusted_splits = text_splitter.split_documents(all_splits[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = vector_db.get_or_create_collection(name=\"book2\")\n",
    "\n",
    "collection.add(\n",
    "    documents=[s.page_content for s in adjusted_splits],\n",
    "    ids=[str(i) for i in range(len(adjusted_splits))],\n",
    "    metadatas=[s.metadata for s in adjusted_splits],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that can answer questions about the provided context.\n",
    "\n",
    "Please cite the chapter used to answer the question. Write the page number in the format \"Chapter X\" at the end of your answer. \n",
    "\n",
    "If the answer is not found in the context, please say so.\n",
    "\"\"\"\n",
    "\n",
    "user_message = \"\"\"\n",
    "Please answer the following question based on the context provided:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@traceable\n",
    "def get_relevant_docs(question: str):\n",
    "    docs = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=3,\n",
    "        include=[\"distances\", \"metadatas\", \"documents\"],\n",
    "    )\n",
    "    documents = docs[\"documents\"][0]\n",
    "    metadatas = docs[\"metadatas\"][0]\n",
    "    return [\n",
    "        {\"page_content\": doc, \"type\": \"Document\", \"metadata\": metadata}\n",
    "        for doc, metadata in zip(documents, metadatas)\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_context(relevant_docs):\n",
    "    context = \"\"\n",
    "    for doc in relevant_docs:\n",
    "        context += f\"--- {doc['metadata']['header_1']}, {doc['metadata']['header_2']} ---\\n{doc['page_content']}\\n\\n\"\n",
    "    return context\n",
    "\n",
    "\n",
    "def get_messages(question: str, relevant_docs: dict):\n",
    "    prompt_vars = {\"question\": question, \"documents\": get_context(relevant_docs)}\n",
    "    messages = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=user_message.format(**prompt_vars)),\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "\n",
    "@traceable\n",
    "def get_response(question: str):\n",
    "    relevant_docs = get_relevant_docs(question)\n",
    "    messages = get_messages(question, relevant_docs)\n",
    "    response = model.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "\n",
    "response = get_response(\"What is the will to Truth?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = get_relevant_docs(\"What is the will to Truth?\")\n",
    "for d in docs:\n",
    "    print(d[\"metadata\"])\n",
    "    print(d[\"page_content\"])\n",
    "    print(\"-\" * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
